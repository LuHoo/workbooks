---
title: "Workshop Chapter 7"
author: "Lucas Hoogduin"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

## Regression analysis

### Exercise 7.1. Before we start

To run the exercises in this workshop, we need to load a few packages.

```{r setup}
library(ggplot2)
library(aicpa)      # Contains US SteamCo data
library(car)
library(gridExtra)  # To create grid of plots
library(tidyr)
library(corrplot)
library(lmtest)
library(latex2exp)  # Use LaTeX expressions in plots
```

### Exercise 7.2. The US SteamCo data file

Throughout this workshop, we work with the `USSteamCo` data file, this is available in the `aicpa` package. The data file is used in the AICPA Guide to Audit Analytics, Exhibit B-3.

The `head` function shows the values of the variables (columns) in the first few observations (rows) from the dataset.

```{r}
head(USSteamCo)
```
### Exercise 7.3. Summarizing and plotting the data

The `summary` command produces summary statistics of all variables in the dataset, the length (number of data points) for character types (such as `month`) and distribution statistics for numeric types.

```{r}
summary(USSteamCo)
```

We create a dummy variable to distinguish between heating months and cooling months. Such a variable would have the value 1 in heating months and 0 in cooling months. The effect of this variable on the trajectory of the regression line is explained in Section 7.4.4.

```{r}
USSteamCo$summer = c(0,0,0,0,1,1,1,1,1,0,0,0)
```
USSteamCo$date = seq(as.Date("2011-01-01"), by = "month", length.out = 48)


We reformat the month variable, that has a character type, into the `date` variable.

```{r}
USSteamCo$date = seq(as.Date("2011-01-01"), 
                     by = "month", length.out = 48)
```

We split the data file into an estimation set and a hold-out set.

```{r}
USSteamCoEstim <- USSteamCo[ 1:36,]
USSteamCoHold  <- USSteamCo[37:48,]
```

Figure 7.2 shows the histograms of the data in the US SteamCo estimation set. 

```{r}
# H7_Histograms
hist_revenue <-
  ggplot(USSteamCoEstim, aes(x=revenue)) + 
  geom_histogram(binwidth = 4000000, fill = "#00338D")
hist_production <- 
   ggplot(USSteamCoEstim, aes(x=production)) + 
  geom_histogram(binwidth = 50000, fill = "#00338D")
hist_coolDD <-
   ggplot(USSteamCoEstim, aes(x=coolDD)) + 
  geom_histogram(binwidth = 50, fill = "#00338D")
hist_heatDD <-
   ggplot(USSteamCoEstim, aes(x=heatDD)) + 
  geom_histogram(binwidth = 100, fill = "#00338D")

grid.arrange(hist_revenue, hist_production, hist_coolDD, hist_heatDD, ncol=2)
```

Figure 7.3 shows the time-series plots for the same dataset. 

```{r}
# Calculate the range of revenue and production
rev_range <- range(USSteamCoEstim$revenue, na.rm = TRUE)
prod_range <- range(USSteamCoEstim$production, na.rm = TRUE)

# Create the plot
ggplot(data = USSteamCoEstim, aes(x = date)) +
  geom_line(aes(y = revenue), colour = "#00338D") +           
  # First time series (revenue)
  geom_line(aes(y = scales::rescale(production, to = rev_range)), 
            colour = "#BC204B") +                             
  # Second time series (production), rescaled to match revenue range
  scale_y_continuous(
    name = "Revenue",                                         
    # Label for the first y-axis
    sec.axis = sec_axis(~scales::rescale(., from = rev_range, to = prod_range), 
                        name = "Production")                  
    # Second y-axis, rescales the plot values back to production's original range
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y.left = element_text(color = "#00338D"),      # Color for first y-axis
    axis.title.y.right = element_text(color = "#BC204B")      # Color for second y-axis
  )

```

Figure 7.1 shows a scatter plot of the independent variable production ($x$-axis) and the dependent variable revenue ($y$-axis) for the Case: US SteamCo. The plot is an easy means to get a feel for the relationship.

```{r}
ussteamco.mod.0 <- lm(revenue ~ production, data = USSteamCoEstim)
ggplot(USSteamCo, aes(x=production, y=revenue)) + 
  geom_point(color = "#00338D") +
    theme_minimal()
```

The enhanced scatter plot in Figure 7.4 is obtained by running the following code.

```{r}
scatterplot(revenue ~ production, 
            data = USSteamCoEstim,
            smooth = FALSE)

```

### Exercise 7.4. The base model `mod.0`

Figure 7.4 shows that there is a positive (increasing) relationship: as production increases, so does revenue. 

We use the function `lm` (for linear model) to estimate the coefficient values of $\beta_0$ and $\beta_1$. This is performed on the data of the estimation set `USSteamCoEstim`, as explained in Section 7.1.1. A brief summary of the model is obtained with the function `brief`.

```{r}
ussteamco.mod.0 <- lm(revenue ~ production, data = USSteamCoEstim)
brief(ussteamco.mod.0)
```
The estimate of the intercept $\beta_0$, $b_0 = 4,897,663$, and the estimate of the slope $\beta_1$, $b_1 = 18.99$.

### Exercise 7.5. The full summary of `mod.0`

A full summary of `mod.0` can be obtained with the `summary` function.

```{r}
summary(ussteamco.mod.0)
```
In addition to the statistics produced by `brief`, it provides insight into the residuals (minimum, first quartile, median, third quartile, and maximum), the $t$ and $p$ values of each of the coefficients and their significance, the adjusted $r^2$, and the $F$ statistic with its $p$ value.

### Exercise 7.6. Correlations and correlogram

The correlations in Table 7.1 are calculated using the `cor` function. 

```{r}
(cor_ussteam <- cor(USSteamCoEstim[, 2:5]))
```
The correlogram in Figure 7.5 is produced by

```{r echo=FALSE}
# H7_Correlogram
corrplot(cor_ussteam, 
         tl.col = "black",
         col=colorRampPalette(c("#bc204b","white","#00348d"))(100),
         col.lim=c(-1,1))
```

### Exercise 7.7. Modelling strategies

The `step` function can be used to implement both backward and forward methods. The `direction` argument specifies whether the stepwise algorithm should proceed forward (`forward`), backward (`backward`), or both (`both`). The `scope` argument specifies the range of models to be searched, whereas the `k` argument may be used to use BIC instead of the default AIC as a criterion.

We start with the forward method. From a model that only uses the constant (`revenue $\sim$ 1`), we add variables one by one. The variable that the largest effect on the criterion, in this example, the default AIC, is selected for inclusion in the model.

```{r}
model_forward <- lm(revenue ~ 1, data = USSteamCoEstim)  # Start with intercept-only model
model_forward <- step(model_forward, 
                      direction = "forward", 
                      scope = formula(~ production + coolDD + heatDD))  
```
Starting with AIC = 1114.66 of a model with only a constant, `R` evaluates the inclusion of each of the available independent variables and their effect on the resulting AIC. In the first step, it appears that including `heatDD` has the best effect on AIC, as it reduces from 1114.66 to 1096.56. 

Using this value of AIC = 1096.56 as the new starting value, `R` evaluates if inclusion of any of the remaining variables has a favorable effect. It finds that including `production` can further reduce the AIC to 1046.11.

In the last step, we see that including `coolDD` does not further reduce the AIC, leading to the final model `revenue $\sim$ heatDD + production`. The summary of this model is:

```{r}
summary(model_forward)  
```

Next, we show an example of the backward method, starting with the full model and eliminating variables one by one.

```{r}
model_backward <- lm(revenue ~ production + coolDD + heatDD, 
                     data = USSteamCoEstim)  
model_backward <- step(model_backward,
                       direction = "backward")  
summary(model_backward)  
```

We observe that this strategy results into the exact same model.

To run the stepwise procedure in both directions, we use the following code. 

```{r}
fit_both <- lm(revenue ~ 1, data = USSteamCoEstim)  
fit_both <- step(fit_both, 
                 direction = "both", 
                 scope = formula(~ production + coolDD + heatDD)) 
summary(fit_both)
```

### Exercise 7.8. Multiple independent variables `mod.1`

We now regress $y$ = `revenue` on $x_1$ = `production`, $x_2$ = `coolDD`, and $x_3$ = `heatDD`.

```{r}
ussteamco.mod.1 <- lm(revenue ~ production + coolDD + heatDD,
                      data = USSteamCoEstim)
summary(ussteamco.mod.1)
```

## Exercise 7.9. Interactions

In Section 7.4.4 we developed `mod.2`, that incorporates two separate regression lines. In addition to the three independent variables, we also account for their interactions with the dummy variable `summer`.

The scatterplot in Figure 7.6 is prepared using

```{r}
scat_rev2 <- scatterplot(revenue ~ production | summer, 
            data = USSteamCoEstim,
            id = list(n = 8, cex = 0.8),
            regLine=TRUE,
            smooth = FALSE
            )
```

Note the special operator `|`, that can be expressed as 'given the value of'. To call this relationship, we use

```{r}
ussteamco.mod.2 <- lm(revenue ~ production * summer + 
                        coolDD * summer +
                        heatDD * summer,
                      data = USSteamCoEstim)
brief(ussteamco.mod.2)
```
The formula specified in the `lm` function, `revenue $\sim$ production * summer + coolDD * summer + heatDD * summer` can also be written as `revenue $\sim$ (production + coolDD + heatDD) * summer`. 

## Exercise 7.10. Time lag

The cross-correlation plot in Figure \ref{fig:H7_timelag} is created by running:

```{r}
with(USSteamCoEstim, 
     ccf(x = production,
         y = revenue,
         lag.max = 4))
```

## Exercise 7.11. Residual plots

The residual plots in Figure \ref{fig:H7_residualPlots_mod2} are obtained by:

```{r}
residualPlots(ussteamco.mod.2)
```

## Exercise 7.12. Influence statistics

Figure 7.10 provides the Influence index plots. From bottom to top: Leverage points, regression outliers, their respective Bonferroni $p$ values and influential observations.

```{r}
influenceIndexPlot(ussteamco.mod.2)
```

## Exercise 7.13. Leverage points

We obtain the hat-values of a model by running the `hatvalues` function on it.

```{r}
hatvals.mod.2 <- hatvalues(ussteamco.mod.2)
```

The three largest hat-values are then summarized by

```{r}
hatvals.mod.2[order(hatvals.mod.2, decreasing = TRUE)[1:5]]
```

The sum of the hat-values is equal to $k + 1$.

```{r}
sum(hatvals.mod.2)
```

## Exercise 7.14. Regression outliers

Figure 7.12 provides the QQ plot for \ttblue{ussteamco.mod.2}, with the three largest Studentized residuals identified.

```{r}
qqPlot(ussteamco.mod.2, id = list(n = 3))
```

## Exercise 7.15. Influential observations

We obtain the Cook's distances of a model by running the `cooks.distance` function on it.

```{r}
cooks.mod.2 <- cooks.distance(ussteamco.mod.2)
```

The three largest Cook's distances are then summarized by

```{r}
cooks.mod.2[order(cooks.mod.2, decreasing = TRUE)[1:3]]
```

## Exercise 7.16. Infuence plot

Figure 7.12 shows the hat-values, Studentized residuals, and Cook’s distances for `ussteamco.mod.2`. The size of the circles is proportional to the Cook’s distance.

```{r}
influencePlot(ussteamco.mod.2, id=list(n=3, cex=0.8))
```

## Exercise 7.17. Winsorizing observation # 22

Define the new estimation set. We replace the original value of observation 22 with its winsorized value.

```{r}
USSteamCoEstim2 <- USSteamCoEstim
p5 <- quantile(ussteamco.mod.2$residuals, 0.05)
fit_22 <- fitted(ussteamco.mod.2)[22]
USSteamCoEstim2$revenue[22] <- fit_22 + p5
```

we then fit a new linear model.

```{r}
ussteamco.mod.3 <- lm(revenue ~ (production + heatDD + coolDD) * summer,
                      data = USSteamCoEstim2)

summary(ussteamco.mod.3)
```

## Exercise 7.18. Multicollinearity

To assess multicollinearity for a model without interactions, like `mod.1`, we use the standard `vif()` function.

```{r}
vif(ussteamco.mod.1)
```

None of the VIF factors is greater than ten, and therefore we conclude that there is no serious inflation of variance.

For models that include interactions, like like `mod.3`, we use the `vif()` function with the additional option `type = 'predictor'`. It then calculates the GVIF instead of the standard VIF.

```{r}
vif(ussteamco.mod.3, type = 'predictor')
```

We now assess whether any of the predictors has a value $GVIF_j^{1/(2df)}$ that is greater than 10. Because there are none, this model does not suffer from variance inflation.

## Exercise 7.19. Variance and TSS

Table 7.5 provides the analysis of variance for `mod.0`. We apply the `anova()` command to perform this analysis.

```{r}
anova(ussteamco.mod.0)
```

The variance of $y$, $s_y^2 = \frac{\sum(y_i - \bar{y})^2}{n - 1}$, is obtained from

```{r}
var(USSteamCoEstim$revenue)
```

Total Sum of Squares $\sum(y_i - \bar{y})^2$ is calculated as

```{r}
(mean_y <- mean(USSteamCoEstim$revenue))
```

```{r}
variation <- USSteamCoEstim$revenue - mean_y
squared_variation <- variation^2
(total_sum_of_squares <- sum(squared_variation))
```

```{r}
anova.mod.0 <- anova(ussteamco.mod.0)
anova.mod.0$`Sum Sq`[1]
anova.mod.0$`Sum Sq`[2]
anova.mod.0$`Sum Sq`[1]/(anova.mod.0$`Sum Sq`[1] + anova.mod.0$`Sum Sq`[2])
```

Therefore, when we divide Total Sum of Squares by $n - 1$, we get the variance of $y$.

```{r}
total_sum_of_squares / 35
```

## Exercise 7.20.anova Type I and Type II

We now turn to a model with multiple independent variables, for example, `mod.1`, introduced in Section 7.4.3.

```{r}
(anova.mod.1 <- anova(ussteamco.mod.1))
```

Declaring the independent variables in a different order shows that the calculation of the sum of squares is in sequential order.

```{r}
ussteamco.mod.1b <- lm(revenue ~ heatDD + coolDD + production,
                       data = USSteamCoEstim)
(anova.mod.1b <- anova(ussteamco.mod.1b))
```
To address this issue, an alternative version of the analysis of variance, referred to as Type II tests, is used. Instead of measuring the contribution of each independent variable sequentially, the analysis shows the contribution of each independent variable if it was the last independent variable added to the model. The analysis is obtained using the command `Anova` (with capital A) from the `car` package.

```{r}
Anova(ussteamco.mod.1)
```

We see that the marginal effect of adding `production`, assuming `coolDD` and `heatDD` have already been entered into the model, is 1.8260e+14. This is equal to the result obtained from the earlier `anova` command on `mod.1b`. We also see that the added value of entering `coolDD` into the model is limited: its sum of squares is much lower than that of the other two variables, and its related $F$ value is not significant. 

We have no clear preference for either `anova` or `Anova`, they both have their merits. `anova` decomposes TSS, but that decomposition is dependent on the order in which variables are entered into the equation. `Anova` shows the marginal effect of each independent variable, but these marginal effects do not add to TSS.

## Exercise 7.21. Anova with interactions

The analysis of variance of `mod.3` is as follows:

```{r}
(anova.mod.3 <- anova(ussteamco.mod.3))
```

## Exercise 7.22. Testing normality

We can assess normality issues with a histogram or with a QQ plot. The histogram in Figure 7.14 is obtained by:

```{r}
nres <- ussteamco.mod.3$residuals / sd(ussteamco.mod.3$residuals)
model3 <- as.data.frame(nres)
hist_stand_res <-
  ggplot(model3, aes(nres)) + 
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(-2, 2, by = 0.5),
                 fill = "#00338D") +
  stat_function(fun = dnorm, args = list(mean = mean(model3$nres), 
                                         sd = sd(model3$nres)))
hist_stand_res
```

The QQ plot is produced by:

```{r}
qqPlot(ussteamco.mod.3)
```


The Shapiro-Wilk normality test is executed through the `shapiro.test` function from the `stats` package.

```{r}
shapiro.test(ussteamco.mod.3$residuals)
```

## Exercise 7.23. Testing heteroskedasticity

To test the asumption of constant variance, we can employ visual means, such as the `residualPlots` in Figure 7.15. We can construct it as follows.

```{r}
residualPlots(ussteamco.mod.3, layout = c(2, 3),
              quadratic = FALSE, linear = TRUE)
```
The formal statistical test is the Breusch-Pagan test.

```{r}
bptest(ussteamco.mod.3)
```

## Exercise 7.24. testing autocorrelation

Figure 7.17: Partial autocorrelation plot for ussteamco.mod.3. A significant spike is observed at lag 1.

```{r}
bgtest(ussteamco.mod.3, order = 3, type = "Chisq")
```
```{r}
pacf(ussteamco.mod.3$residuals)
```

The $p$ value of the test statistic is 0.06737. Therefore, we reject the null hypothesis at levels below 0.06737, suggesting marginal evidence of autocorrelation in the residuals. 

## Exercise 7.25. The Cochrane-Orcutt method

A practical and widely used fix is the `Cochrane-Orcutt method`, designed specifically to correct for first-order autocorrelation (AR(1)). 
This method estimates the autocorrelation parameter and transforms the regression model by subtracting a multiple of lagged values from both the dependent and independent variables. 
The transformed model can then be estimated by ordinary least squares, restoring valid inference under autocorrelation.

We first estimate an AR(1) model to the residuals from `mod.3`

```{r}
(rho <- arima(ussteamco.mod.3$residuals, 
              order = c(1, 0, 0))$coef[1])
```
Then we transform all variables using the estimated $\rho$.

```{r}
# Create lagged variables (dropping first observation)
n <- nrow(USSteamCoEstim2)

# Lag all necessary variables by 1 time unit
trans <- USSteamCoEstim2[-1, ]  # t
lag       <- USSteamCoEstim2[-n, ]  # t-1

# Transform dependent variable
trans$revenue_adj <- trans$revenue - rho * lag$revenue

# Transform independent variables
trans$production_adj <- trans$production - rho * lag$production
trans$heatDD_adj     <- trans$heatDD     - rho * lag$heatDD
trans$coolDD_adj     <- trans$coolDD     - rho * lag$coolDD
trans$summer_adj     <- trans$summer     - rho * lag$summer

# Interaction terms
trans$prod_summer_adj   <- trans$production_adj * trans$summer_adj
trans$heat_summer_adj   <- trans$heatDD_adj * trans$summer_adj
trans$cool_summer_adj   <- trans$coolDD_adj * trans$summer_adj
```

Fit a new model using the Cochrane-Orcutt method

```{r}
ussteamco.mod.4 <- lm(revenue_adj ~ production_adj + heatDD_adj + coolDD_adj +
                                     summer_adj + prod_summer_adj +
                                     heat_summer_adj + cool_summer_adj,
                      data = trans)

summary(ussteamco.mod.4)
```
This manual approach closely follows the Cochrane-Orcutt transformation:
$y_t - \rho y_{t-1} = \beta_0(1 - \rho) + \sum_{j} \beta_j(x_{jt} - \rho x_{j,t-1}) + u_t$
It discards the first observation (which cannot be differenced).
After fitting the transformed model, we can check if autocorrelation is reduced:

```{r}
pacf(ussteamco.mod.4$residuals)
bgtest(ussteamco.mod.4, order = 3)
```


## Exercise 7.26. Testing significance

Significance of each of the independent variables and their interactions can be read from the summary() table.

```{r}
options(scipen = 1)
summary(ussteamco.mod.4)
```

## Exercise 7.27 Manual backward elimination.

We can apply a stepwise model selection procedure (such as BIC-based backward elimination) to simplify our Cochrane-Orcutt–transformed model by removing non-significant predictors.
However, there’s a caveat: because we have manually transformed the variables (e.g., `production_adj`, `prod_summer_adj`, etc.), stepwise selection must be applied manually to the transformed model — you can’t just call `step()` on `ussteamco.mod.4` without interpreting carefully what it does.

```{r}
library(MASS)

stepwise_model <- stepAIC(ussteamco.mod.3, direction = "both", trace = TRUE, k = log(n))
summary(stepwise_model)
```
```{r}
pacf(stepwise_model$residuals)
bgtest(stepwise_model, order = 3)
```



```{r}
summary(ussteamco.mod.3)
```
Derivation of adjusted r-squared
```{r}
(rsq <- summary(ussteamco.mod.3)$r.squared)
(k <- 7)
(adjrsq <- rsq - k * (1 - rsq) / 27)
```



Looks like model without `heatDD` works better.

```{r}
ussteamco.mod.5 <- lm(revenue ~ 
                        log(production) * summer + 
                        coolDD * summer,
                      data = USSteamCoEstim2)
summary(ussteamco.mod.5)
```
Compare two models with `BIC` values.

```{r}
BIC(ussteamco.mod.0)
# BIC(ussteamco.mod.1)
# BIC(ussteamco.mod.2)
# BIC(ussteamco.mod.3)
# BIC(ussteamco.mod.4)
# BIC(ussteamco.mod.5)
```
Reconciliation of BIC values
```{r}
# Extract number of parameters (including intercept)
(k <- length(coef(ussteamco.mod.0)))

# Compute SSE (sum of squared errors)
(sse <- sum(residuals(ussteamco.mod.0)^2))

# Compute manual BIC using SSE-based formula
(bic_sse <- n * log(sse / n) + k * log(n))

# Compute BIC using R's built-in log-likelihood-based formula
(bic_r <- BIC(ussteamco.mod.0))

# Estimate of sigma^2 (MLE, not unbiased)
(sigma2_hat <- sse / n)

# Compute log-likelihood at MLE
(logL_hat <- -n / 2 * (log(2 * pi) + 1 + log(sse / n)))

# Compute BIC using full log-likelihood
(bic_logL <- -2 * logL_hat + k * log(n))


```
```{r}
(n <- length(residuals(ussteamco.mod.0)))
```
```{r}
(k_r <- attr(logLik(ussteamco.mod.0), "df"))
length(coef(ussteamco.mod.0))   # Your k
attr(logLik(ussteamco.mod.0), "df")   # What R uses
```
```{r}
(logL_r <- as.numeric(logLik(ussteamco.mod.0)))
(bic_from_logLik <- -2 * logL_r + k_r * log(n))
```
```{r}
# Sample size used in the model
n <- length(residuals(ussteamco.mod.0))

# Number of parameters used by R internally
k_r <- attr(logLik(ussteamco.mod.0), "df")

# SSE
sse <- sum(residuals(ussteamco.mod.0)^2)

# BIC from your SSE-based formula
bic_sse <- n * log(sse / n) + k_r * log(n)

# BIC from R's actual log-likelihood
logL_r <- as.numeric(logLik(ussteamco.mod.0))
bic_from_logLik <- -2 * logL_r + k_r * log(n)

# Built-in BIC
bic_r <- BIC(ussteamco.mod.0)

# Compare
cat("Your SSE-based BIC:         ", bic_sse, "\n")
cat("BIC from R logLik() formula:", bic_from_logLik, "\n")
cat("R's BIC():                  ", bic_r, "\n")
```
```{r}
n * (log(2 * pi) + 1)
bic_from_logLik - bic_sse
```
This is the difference between 1105.22 and 1207.383


```{r}
Anova(ussteamco.mod.5)
```

```{r}
bgtest(ussteamco.mod.5, order = 24)
```

Most of the autocorrelation has vanished when we take `log(production)` and leave out `heatDD`. But testing for up to 12 lags still shows a problem (seasonality?)

```{r}
pacf(ussteamco.mod.5$residuals)
```



# Predictions

```{r}
USSteamCoHold$predmod5 <- predict(ussteamco.mod.5, newdata = USSteamCoHold)
USSteamCoHold$diffmod5 <- USSteamCoHold$revenue - USSteamCoHold$predmod5
```

```{r echo=FALSE}
# 
ggplot(USSteamCoHold, aes(x=production, y=diffmod5)) + 
  geom_point(color = "#00338D")
```

Discuss that most differences are negative.

# Auditor response
## Precision and acceptable diﬀerence
## Identification, investigation, and evaluation
## Amount of audit evidence obtained
